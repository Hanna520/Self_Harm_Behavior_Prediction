{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suicidal and Self-Injurious Incidents Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script runs the Transformer Encoder model on notes data alone and on notes alongside structured data with and without under-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For handeling dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import re                                  # For regular expression operations\n",
    "import string                              # For string operations\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk                                # Natural Language Toolkit\n",
    "from nltk.corpus import stopwords          # For stop words that come with NLTK\n",
    "\n",
    "\n",
    "# For building neural netwrok models\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# For model evaluation\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# For text agumenters\n",
    "import os\n",
    "os.environ[\"MODEL_DIR\"] = '../model'\n",
    "import nlpaug\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "\n",
    "from nlpaug.util import Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87975, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load notes data for positive cases\n",
    "# From R script \"str_notes_4k_Alex\"\n",
    "data = pd.read_csv(\"Z:/rscripts/users/interns/hlu/data/cleaned_model_str_notes_Alex.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data['event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['asses_plan'] = data['soap_ass'] + data['soap_plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 0', 'InmateID','DateOfBirth', 'BookingDate', 'LastUpdateDateTime', 'ReleaseDate','soap_ass', 'soap_plan'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = pd.read_csv(\"Z:/rscripts/users/interns/hlu/Abbreviations2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = dict(zip(abbreviations.abbreviations, abbreviations.complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(text):\n",
    "    text = \"\".join([x.lower() for x in text]) # Convert to lower case\n",
    "    text = re.sub(\"(___|\\+|nan)\", \" \", text).strip() \n",
    "    text = \" \".join([df_dict.get(wrd, wrd) for wrd in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['soap_sub'] = data['soap_sub'].map(str).apply(replace_words)\n",
    "data['soap_obj'] = data['soap_obj'].map(str).apply(replace_words)\n",
    "data['asses_plan'] = data['asses_plan'].map(str).apply(replace_words)\n",
    "data['quick_notes'] = data['quick_notes'].map(str).apply(replace_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics Before Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before cleaning for soap_sub\n",
      "Quantiles:  [0, 18, 56, 163, 22025]\n",
      "Mean:  210\n",
      "Standard deviation:  599\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of the number of words after cleaning\n",
    "print(\"Number of words before cleaning for soap_sub\")\n",
    "print(\"Quantiles: \", [int(e) for e in (np.quantile([len(x.split()) for x in data[\"soap_sub\"]], q = [0, 0.25, 0.5, 0.75, 1]))])\n",
    "print(\"Mean: \", round(np.mean([len(x.split()) for x in data[\"soap_sub\"]])))\n",
    "print(\"Standard deviation: \", round(np.std([len(x.split()) for x in data[\"soap_sub\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before cleaning for soap_obj\n",
      "Quantiles:  [0, 16, 55, 148, 23987]\n",
      "Mean:  174\n",
      "Standard deviation:  504\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of the number of words after cleaning\n",
    "print(\"Number of words before cleaning for soap_obj\")\n",
    "print(\"Quantiles: \", [int(e) for e in (np.quantile([len(x.split()) for x in data[\"soap_obj\"]], q = [0, 0.25, 0.5, 0.75, 1]))])\n",
    "print(\"Mean: \", round(np.mean([len(x.split()) for x in data[\"soap_obj\"]])))\n",
    "print(\"Standard deviation: \", round(np.std([len(x.split()) for x in data[\"soap_obj\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before cleaning for assessment and plan\n",
      "Quantiles:  [0, 33, 57, 146, 33419]\n",
      "Mean:  186\n",
      "Standard deviation:  512\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of the number of words after cleaning\n",
    "print(\"Number of words before cleaning for assessment and plan\")\n",
    "print(\"Quantiles: \", [int(e) for e in (np.quantile([len(x.split()) for x in data[\"asses_plan\"]], q = [0, 0.25, 0.5, 0.75, 1]))])\n",
    "print(\"Mean: \", round(np.mean([len(x.split()) for x in data[\"asses_plan\"]])))\n",
    "print(\"Standard deviation: \", round(np.std([len(x.split()) for x in data[\"asses_plan\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before cleaning for quick notes\n",
      "Quantiles:  [1, 52, 116, 209, 100117]\n",
      "Mean:  243\n",
      "Standard deviation:  866\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of the number of words after cleaning\n",
    "print(\"Number of words before cleaning for quick notes\")\n",
    "print(\"Quantiles: \", [int(e) for e in (np.quantile([len(x.split()) for x in data[\"quick_notes\"]], q = [0, 0.25, 0.5, 0.75, 1]))])\n",
    "print(\"Mean: \", round(np.mean([len(x.split()) for x in data[\"quick_notes\"]])))\n",
    "print(\"Standard deviation: \", round(np.std([len(x.split()) for x in data[\"quick_notes\"]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\hongxia\n",
      "[nltk_data]     lu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Import the standard English stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def preprocess(text):\n",
    "    text = \"\".join([x for x in text if x not in string.punctuation]) # Remmove punctuations \n",
    "    text = ' '.join(['' if (x in stopwords_english) else x for x in text.split()]) # Remove stopwords\n",
    "    text = re.sub(\"(\\W|\\d+|\\n)\", \" \", text).strip() # remove spaces, digits and line breaks\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BookingNumber', 'Sex', 'Race', 'MaritalStatus', 'soap_sub', 'soap_obj',\n",
       "       'quick_notes', 'event', 'age', 'AB109', 'asses_plan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"soap_sub\"] = data[\"soap_sub\"].apply(preprocess)\n",
    "data[\"soap_obj\"] = data[\"soap_obj\"].apply(preprocess)\n",
    "data[\"quick_notes\"] = data[\"quick_notes\"].apply(preprocess)\n",
    "data[\"asses_plan\"] = data[\"asses_plan\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after cleaning\n",
      "Quantiles:  [    0.    14.    37.   102. 12406.]\n",
      "Mean:  131\n",
      "Median:  37\n",
      "Standard deviation:  366\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of the number of words after cleaning\n",
    "print(\"Number of words after cleaning\")\n",
    "# print(\"Quantiles: \", np.round(np.quantile([len(x.split()) for x in clean_sub], q = [0, 0.25, 0.5, 0.75, 1])))\n",
    "print(\"Quantiles: \", np.round(np.quantile([len(x.split()) for x in data[\"soap_sub\"]], q = [0, 0.25, 0.5, 0.75, 1])))\n",
    "print(\"Mean: \", round(np.mean([len(x.split()) for x in data[\"soap_sub\"]])))\n",
    "print(\"Median: \", round(np.median([len(x.split()) for x in data[\"soap_sub\"]])))\n",
    "print(\"Standard deviation: \", round(np.std([len(x.split()) for x in data[\"soap_sub\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_obj = data[\"soap_obj\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after cleaning\n",
      "Quantiles:  [0, 11, 44, 110, 16828]\n",
      "Mean:  129\n",
      "Median:  44\n",
      "Standard deviation:  370\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of the number of words after cleaning\n",
    "print(\"Number of words after cleaning\")\n",
    "# print(\"Quantiles: \", [int(e) for e in (np.round(np.quantile([len(x.split()) for x in clean_obj], q = [0, 0.25, 0.5, 0.75, 1])))])\n",
    "print(\"Quantiles: \", [int(e) for e in (np.round(np.quantile([len(x.split()) for x in data[\"soap_obj\"]], q = [0, 0.25, 0.5, 0.75, 1])))])\n",
    "print(\"Mean: \", round(np.mean([len(x.split()) for x in data[\"soap_obj\"]])))\n",
    "print(\"Median: \", round(np.median([len(x.split()) for x in data[\"soap_obj\"]])))\n",
    "print(\"Standard deviation: \", round(np.std([len(x.split()) for x in data[\"soap_obj\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after cleaning\n",
      "Quantiles:  [0, 19, 41, 101, 21795]\n",
      "Mean:  129\n",
      "Median:  41\n",
      "Standard deviation:  349\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of the number of words after cleaning\n",
    "print(\"Number of words after cleaning\")\n",
    "# print(\"Quantiles: \", [int(e) for e in (np.quantile([len(x.split()) for x in clean_asplan], q = [0, 0.25, 0.5, 0.75, 1]))])\n",
    "print(\"Quantiles: \", [int(e) for e in (np.quantile([len(x.split()) for x in data[\"asses_plan\"]], q = [0, 0.25, 0.5, 0.75, 1]))])\n",
    "print(\"Mean: \", round(np.mean([len(x.split()) for x in data[\"asses_plan\"]])))\n",
    "print(\"Median: \", round(np.median([len(x.split()) for x in data[\"asses_plan\"]])))\n",
    "print(\"Standard deviation: \", round(np.std([len(x.split()) for x in data[\"asses_plan\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after cleaning\n",
      "Quantiles:  [1, 36, 82, 143, 66513]\n",
      "Mean:  162\n",
      "Median:  82\n",
      "Standard deviation:  564\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics of the number of words after cleaning\n",
    "print(\"Number of words after cleaning\")\n",
    "# print(\"Quantiles: \", [int(e) for e in (np.quantile([len(x.split()) for x in clean_quick], q = [0, 0.25, 0.5, 0.75, 1]))])\n",
    "print(\"Quantiles: \", [int(e) for e in (np.quantile([len(x.split()) for x in data[\"quick_notes\"]], q = [0, 0.25, 0.5, 0.75, 1]))])\n",
    "print(\"Mean: \", round(np.mean([len(x.split()) for x in data[\"quick_notes\"]])))\n",
    "print(\"Median: \", round(np.median([len(x.split()) for x in data[\"quick_notes\"]])))\n",
    "print(\"Standard deviation: \", round(np.std([len(x.split()) for x in data[\"quick_notes\"]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    pred = (y_pred > 0.5).astype(\"int32\")\n",
    "#     acc = np.sum(y == pred)/len(pred)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = (2*precision*recall)/(precision + recall)\n",
    "    acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "    \n",
    "    auc_roc = round(roc_auc_score(y, y_pred),4)\n",
    "    pre, rec, thresholds = precision_recall_curve(y, y_pred)\n",
    "    auc_pr = round(auc(rec, pre),4)\n",
    "    return(auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = np.sum(data['event'])/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028303495311167944"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevalence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Transformer model\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, max_length, vocab_size, embedding_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=max_length, output_dim=embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "#         maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=max_length, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.3):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embedding_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "\n",
    "# Four inputs (all texts)\n",
    "# Three inputs (flatten before concatenating)\n",
    "def transformer_model_4_flatten(vocab_size, embedding_dim,max_length,dropout_rate):   \n",
    "    inputs = layers.Input(shape=(max_length,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(max_length,vocab_size, embedding_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    flat_1 = layers.Flatten()(x)\n",
    "    \n",
    "    inputs_2 = layers.Input(shape=(max_length,))\n",
    "    embedding_layer_2 = TokenAndPositionEmbedding(max_length,vocab_size, embedding_dim)\n",
    "    x_2 = embedding_layer_2(inputs_2)\n",
    "    transformer_block_2 = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    x_2 = transformer_block_2(x_2)\n",
    "    x_2 = layers.GlobalAveragePooling1D()(x_2)\n",
    "    x_2 = layers.Dropout(dropout_rate)(x_2)\n",
    "    flat_2 = layers.Flatten()(x_2)\n",
    "    \n",
    "    \n",
    "    inputs_3 = layers.Input(shape=(max_length,))\n",
    "    embedding_layer_3 = TokenAndPositionEmbedding(max_length,vocab_size, embedding_dim)\n",
    "    x_3 = embedding_layer_3(inputs_3)\n",
    "    transformer_block_3 = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    x_3 = transformer_block_3(x_3)\n",
    "    x_3 = layers.GlobalAveragePooling1D()(x_3)\n",
    "    x_3 = layers.Dropout(dropout_rate)(x_3)\n",
    "    flat_3 = layers.Flatten()(x_3)\n",
    "    \n",
    "    inputs_4 = layers.Input(shape=(max_length,))\n",
    "    embedding_layer_4 = TokenAndPositionEmbedding(max_length,vocab_size, embedding_dim)\n",
    "    x_4 = embedding_layer_4(inputs_4)\n",
    "    transformer_block_4 = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    x_4 = transformer_block_4(x_4)\n",
    "    x_4 = layers.GlobalAveragePooling1D()(x_4)\n",
    "    x_4 = layers.Dropout(dropout_rate)(x_4)\n",
    "    flat_4 = layers.Flatten()(x_4)\n",
    "    \n",
    "    merge = concatenate([flat_1, flat_2, flat_3, flat_4])\n",
    "    \n",
    "    hidden = Dense(128, activation='relu')(merge)\n",
    "    \n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs, inputs_2, inputs_3, inputs_4], outputs=outputs)\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['AUC'])\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "\n",
    "# Four text inputs + sturctured (flatten before concatenating)\n",
    "def transformer_model_5_structured(vocab_size, embedding_dim,max_length,dropout_rate):   \n",
    "    inputs = layers.Input(shape=(max_length,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(max_length,vocab_size, embedding_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    flat_1 = layers.Flatten()(x)\n",
    "    \n",
    "    inputs_2 = layers.Input(shape=(max_length,))\n",
    "    embedding_layer_2 = TokenAndPositionEmbedding(max_length,vocab_size, embedding_dim)\n",
    "    x_2 = embedding_layer_2(inputs_2)\n",
    "    transformer_block_2 = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    x_2 = transformer_block_2(x_2)\n",
    "    x_2 = layers.GlobalAveragePooling1D()(x_2)\n",
    "    x_2 = layers.Dropout(dropout_rate)(x_2)\n",
    "    flat_2 = layers.Flatten()(x_2)\n",
    "    \n",
    "    \n",
    "    inputs_3 = layers.Input(shape=(max_length,))\n",
    "    embedding_layer_3 = TokenAndPositionEmbedding(max_length,vocab_size, embedding_dim)\n",
    "    x_3 = embedding_layer_3(inputs_3)\n",
    "    transformer_block_3 = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    x_3 = transformer_block_3(x_3)\n",
    "    x_3 = layers.GlobalAveragePooling1D()(x_3)\n",
    "    x_3 = layers.Dropout(dropout_rate)(x_3)\n",
    "    flat_3 = layers.Flatten()(x_3)\n",
    "    \n",
    "    \n",
    "    inputs_4 = layers.Input(shape=(max_length,))\n",
    "    embedding_layer_4 = TokenAndPositionEmbedding(max_length,vocab_size, embedding_dim)\n",
    "    x_4 = embedding_layer_4(inputs_4)\n",
    "    transformer_block_4 = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "    x_4 = transformer_block_4(x_4)\n",
    "    x_4 = layers.GlobalAveragePooling1D()(x_4)\n",
    "    x_4 = layers.Dropout(dropout_rate)(x_4)\n",
    "    flat_4 = layers.Flatten()(x_4)\n",
    "    \n",
    "    inputs_5 = layers.Input(shape=(19,)) # Number of structured variables\n",
    "    x_5 = Dense(64, activation='relu')(inputs_5)\n",
    "    x_5 = layers.Dropout(dropout_rate)(x_5)\n",
    "    x_5 = Dense(32, activation='relu')(x_5)\n",
    "    x_5 = layers.Dropout(dropout_rate)(x_5)\n",
    "    \n",
    "    \n",
    "    merge = concatenate([flat_1, flat_2, flat_3, flat_4, x_5])\n",
    "    \n",
    "    hidden = Dense(128, activation='relu')(merge)\n",
    "    \n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs, inputs_2, inputs_3, inputs_4, inputs_5], outputs=outputs)\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['AUC'])\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four Inputs (all notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running model, \n",
    "### 1. split data into 10 training and test sets\n",
    "### 2. keep the 10 original test set\n",
    "### 3. under-sample trainng sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 10 copies of the indices of the training and test sets so that all models are compared on the same training and test sets \n",
    "# train_idx_10 = []\n",
    "# test_idx_10 = []\n",
    "\n",
    "# sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
    "\n",
    "# for train_index, test_index in sss.split(data['soap_obj'], data['event']):\n",
    "#     train_idx_10.append(train_index)\n",
    "#     test_idx_10.append(test_index)\n",
    "    \n",
    "# pd_train_idx_10 = pd.DataFrame(np.array(train_idx_10).reshape(np.array(train_idx_10).shape[0], np.array(train_idx_10).shape[1]))\n",
    "# pd_train_idx_10 = pd_train_idx_10.transpose()\n",
    "\n",
    "# pd_test_idx_10 = pd.DataFrame(np.array(test_idx_10).reshape(np.array(test_idx_10).shape[0], np.array(test_idx_10).shape[1]))\n",
    "# pd_test_idx_10 = pd_test_idx_10.transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the indices in case the training is interrupted due to forced shut-down of the computer\n",
    "# pd_train_idx_10.to_csv('other/pd_train_idx_10.csv')\n",
    "# pd_test_idx_10.to_csv('other/pd_test_idx_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test indices\n",
    "pd_train_idx_10 = pd.read_csv('other/pd_train_idx_10.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21382</td>\n",
       "      <td>67168</td>\n",
       "      <td>44241</td>\n",
       "      <td>32895</td>\n",
       "      <td>50243</td>\n",
       "      <td>16907</td>\n",
       "      <td>59817</td>\n",
       "      <td>84579</td>\n",
       "      <td>42076</td>\n",
       "      <td>26331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9973</td>\n",
       "      <td>21571</td>\n",
       "      <td>52839</td>\n",
       "      <td>8788</td>\n",
       "      <td>67684</td>\n",
       "      <td>29912</td>\n",
       "      <td>11357</td>\n",
       "      <td>46855</td>\n",
       "      <td>26480</td>\n",
       "      <td>55932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      0      1      2      3      4      5      6      7      8  \\\n",
       "0           0  21382  67168  44241  32895  50243  16907  59817  84579  42076   \n",
       "1           1   9973  21571  52839   8788  67684  29912  11357  46855  26480   \n",
       "\n",
       "       9  \n",
       "0  26331  \n",
       "1  55932  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train_idx_10.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train_idx_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes Only without Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No augmentation No undersampling\n",
    "epochs = 20 \n",
    "batch_size = 32\n",
    "max_length = 162 # Largest mean length of the four types of notes\n",
    "\n",
    "dropout_rate = 0.3\n",
    "embedding_dim = 200\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "time_1 = time.time()\n",
    "\n",
    "\n",
    "X = data['soap_obj']\n",
    "X_2 = data['soap_sub']\n",
    "X_3 = data['asses_plan']\n",
    "X_4 = data['quick_notes']\n",
    "y = data['event']\n",
    "\n",
    "\n",
    "j = 0 # to keep track of the iteration number\n",
    "time_start = time.time()\n",
    "\n",
    "with open('other/soap_self_harm.csv','a') as fd:\n",
    "    fd.write(f'No augmentation_4 heads_20epochs_replace_words_4-text-inputs_All_Save_Pred_startover\\n')\n",
    "    \n",
    "pred_train_10 = [] \n",
    "pred_test_10 = []\n",
    "\n",
    "    \n",
    "for ii in range(10):\n",
    "    time_s = time.time()\n",
    "    \n",
    "    # Train and test data\n",
    "    train_index = pd_train_idx_10.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    j += 1\n",
    "    iteration = \"iter\" + str(j)\n",
    "    x_train_1, x_test_1 = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    x_train_2, x_test_2 = X_2[train_index], X_2[test_index]\n",
    "    x_train_3, x_test_3 = X_3[train_index], X_3[test_index]\n",
    "    x_train_4, x_test_4 = X_4[train_index], X_4[test_index]\n",
    "\n",
    "    # Run model\n",
    "    model_time_start = time.time()    \n",
    "\n",
    "    # Tokenize the text   \n",
    "        \n",
    "    tokenizer = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer.fit_on_texts(x_train_1)\n",
    "    x_train_1 = tokenizer.texts_to_sequences(x_train_1)\n",
    "    x_test_1 = tokenizer.texts_to_sequences(x_test_1)\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # plus the reserved index 0\n",
    "    word_index = tokenizer.word_index\n",
    "    del tokenizer  \n",
    "    \n",
    "    tokenizer_2 = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer_2.fit_on_texts(x_train_2)\n",
    "    x_train_2 = tokenizer_2.texts_to_sequences(x_train_2)\n",
    "    x_test_2 = tokenizer_2.texts_to_sequences(x_test_2)\n",
    "    del tokenizer_2\n",
    "    \n",
    "    tokenizer_3 = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer_3.fit_on_texts(x_train_3)\n",
    "    x_train_3 = tokenizer_3.texts_to_sequences(x_train_3)\n",
    "    x_test_3 = tokenizer_3.texts_to_sequences(x_test_3)\n",
    "    del tokenizer_3\n",
    "     \n",
    "    tokenizer_4 = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer_4.fit_on_texts(x_train_4)\n",
    "    x_train_4 = tokenizer_4.texts_to_sequences(x_train_4)\n",
    "    x_test_4 = tokenizer_4.texts_to_sequences(x_test_4)\n",
    "    del tokenizer_4\n",
    "\n",
    "    # Pad the sequences with 0's\n",
    "    x_train_1 = pad_sequences(x_train_1, padding='post', maxlen=max_length) \n",
    "    x_test_1 = pad_sequences(x_test_1, padding='post', maxlen=max_length)\n",
    "    \n",
    "    x_train_2 = pad_sequences(x_train_2, padding='post', maxlen=max_length) \n",
    "    x_test_2 = pad_sequences(x_test_2, padding='post', maxlen=max_length)\n",
    "    \n",
    "    x_train_3 = pad_sequences(x_train_3, padding='post', maxlen=max_length) \n",
    "    x_test_3 = pad_sequences(x_test_3, padding='post', maxlen=max_length)\n",
    "       \n",
    "    x_train_4 = pad_sequences(x_train_4, padding='post', maxlen=max_length) \n",
    "    x_test_4 = pad_sequences(x_test_4, padding='post', maxlen=max_length)\n",
    "\n",
    "    # Fit the Transformer model\n",
    "    \n",
    "#     snap1 = tracemalloc.take_snapshot()\n",
    "    \n",
    "    mymodel = transformer_model_4_flatten(vocab_size, embedding_dim,max_length,dropout_rate)\n",
    "    mymodel.fit([x_train_1,x_train_2,x_train_3,x_train_4], y_train, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    # Collect and log evaluation metrics\n",
    "    auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(mymodel, [x_test_1,x_test_2,x_test_3,x_test_4], y_test)\n",
    "    model_time = time.time() - model_time_start\n",
    "\n",
    "    with open('other/soap_self_harm.csv','a') as fd:\n",
    "        fd.write(f'{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{model_time},{tn},{fp},{fn},{tp}\\n')\n",
    "\n",
    "    # Save predictions\n",
    "    pred_train = mymodel.predict([x_train_1,x_train_2,x_train_3,x_train_4])    \n",
    "    pred_train_10.append(pred_train)\n",
    "    np.save('other/predictions/' + 'pred_train_' + str(ii), pred_train)\n",
    "    \n",
    "    pred_test = mymodel.predict([x_test_1,x_test_2,x_test_3,x_test_4])\n",
    "    pred_test_10.append(pred_test)\n",
    "    np.save('other/predictions/' + 'pred_test_' + str(ii), pred_test)\n",
    "    \n",
    "    del mymodel,x_train_1,x_train_2,x_train_3,x_train_4,x_test_1,x_test_2,x_test_3,x_test_4\n",
    "    gc.collect()\n",
    "    \n",
    "    time_e = time.time() - time_s\n",
    "    with open('other/soap_self_harm.csv','a') as fd:\n",
    "        fd.write(f'1 iteration 18 DA,{time_e}\\n')\n",
    "\n",
    "running_time = time.time() - time_start\n",
    "with open('other/soap_self_harm.csv','a') as fd:\n",
    "        fd.write(f'10 iteration training time,{running_time}\\n')\n",
    "        \n",
    "# Save Predicted Probabilities       \n",
    "pd_pred_train_10 = pd.DataFrame(np.array(pred_train_10).reshape(np.array(pred_train_10).shape[0],np.array(pred_train_10).shape[1]))\n",
    "pd_pred_train_10 = pd_pred_train_10.transpose()\n",
    "\n",
    "pd_pred_test_10 = pd.DataFrame(np.array(pred_test_10).reshape(np.array(pred_test_10).shape[0],np.array(pred_test_10).shape[1]))\n",
    "pd_pred_test_10 = pd_pred_test_10.transpose()\n",
    "\n",
    "pd_pred_train_10.to_csv(\"other2\\pd_pred_train_10.csv\")\n",
    "pd_pred_test_10.to_csv(\"other2\\pd_pred_test_10.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes Only with Under-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No augmentation No undersampling\n",
    "epochs = 20 \n",
    "batch_size = 32\n",
    "max_length = 162 # Largest mean length of the four types of notes\n",
    "\n",
    "dropout_rate = 0.3\n",
    "embedding_dim = 200\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "time_1 = time.time()\n",
    "\n",
    "X = data['soap_obj']\n",
    "X_2 = data['soap_sub']\n",
    "X_3 = data['asses_plan']\n",
    "X_4 = data['quick_notes']\n",
    "y = data['event']\n",
    "\n",
    "# del data\n",
    "\n",
    "j = 0 # to keep track of the iteration number\n",
    "time_start = time.time()\n",
    "\n",
    "pct = 0.5 # Percent of positive cases after undersampling\n",
    "\n",
    "with open('other/soap_self_harm.csv','a') as fd:\n",
    "    fd.write(f'No augmentation_4 heads_20epochs_replace_words_4-text-inputs_Undersample to {pct} prevalence\\n')\n",
    "\n",
    "pred_train_10 = [] \n",
    "pred_test_10 = []\n",
    "train_idx_under_10 = []\n",
    "\n",
    "for ii in range(10):\n",
    "    time_s = time.time()\n",
    "    \n",
    "    j += 1\n",
    "    iteration = \"iter\" + str(j)\n",
    "    \n",
    "    # Train and test data\n",
    "    train_index = pd_train_idx_10.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    \n",
    "    # Undersample the negative cases of the training set, no changes to test sets\n",
    "    y_train_neg = y[train_index][y[train_index]==0]\n",
    "    y_train_pos = y[train_index][y[train_index]==1]\n",
    "\n",
    "    num_samples = round((1-pct)/pct*len(y_train_pos)) # Number of samples needed from the negatives\n",
    "    samples = y_train_neg.sample(n=num_samples, replace=False, random_state=0) # Downsample training set\n",
    "    \n",
    "    y_train_neg = y_train_neg[samples.index]\n",
    "    train_idx_under = pd.concat([y_train_neg, y_train_pos], axis=0).index # Indices for undersmpled training set\n",
    "    train_idx_under_10.append(train_idx_under)\n",
    "      \n",
    "    # save the under-sampled indices for training\n",
    "    path = 'undersampled_idices/train_idx_pct_' + str(pct)+ '/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    np.save(path + 'train_idx_' + str(ii), train_idx_under)\n",
    "    \n",
    "    x_train_1, x_test_1 = X[train_idx_under], X[test_index]\n",
    "    y_train, y_test = y[train_idx_under], y[test_index]\n",
    "\n",
    "    x_train_2, x_test_2 = X_2[train_idx_under], X_2[test_index]\n",
    "    x_train_3, x_test_3 = X_3[train_idx_under], X_3[test_index]\n",
    "    x_train_4, x_test_4 = X_4[train_idx_under], X_4[test_index]\n",
    "\n",
    "    # Run model\n",
    "    model_time_start = time.time()\n",
    "\n",
    "\n",
    "    # Tokenize the text   \n",
    "    tokenizer = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer.fit_on_texts(x_train_1)\n",
    "    x_train_1 = tokenizer.texts_to_sequences(x_train_1)\n",
    "    x_test_1 = tokenizer.texts_to_sequences(x_test_1)\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # plus the reserved index 0\n",
    "    word_index = tokenizer.word_index\n",
    "    del tokenizer  \n",
    "    \n",
    "    tokenizer_2 = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer_2.fit_on_texts(x_train_2)\n",
    "    x_train_2 = tokenizer_2.texts_to_sequences(x_train_2)\n",
    "    x_test_2 = tokenizer_2.texts_to_sequences(x_test_2)\n",
    "    del tokenizer_2\n",
    "    \n",
    "    tokenizer_3 = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer_3.fit_on_texts(x_train_3)\n",
    "    x_train_3 = tokenizer_3.texts_to_sequences(x_train_3)\n",
    "    x_test_3 = tokenizer_3.texts_to_sequences(x_test_3)\n",
    "    del tokenizer_3\n",
    "     \n",
    "    tokenizer_4 = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer_4.fit_on_texts(x_train_4)\n",
    "    x_train_4 = tokenizer_4.texts_to_sequences(x_train_4)\n",
    "    x_test_4 = tokenizer_4.texts_to_sequences(x_test_4)\n",
    "    del tokenizer_4\n",
    "\n",
    "    # Pad the sequences with 0's\n",
    "    x_train_1 = pad_sequences(x_train_1, padding='post', maxlen=max_length) \n",
    "    x_test_1 = pad_sequences(x_test_1, padding='post', maxlen=max_length)\n",
    "    \n",
    "    x_train_2 = pad_sequences(x_train_2, padding='post', maxlen=max_length) \n",
    "    x_test_2 = pad_sequences(x_test_2, padding='post', maxlen=max_length)\n",
    "    \n",
    "    x_train_3 = pad_sequences(x_train_3, padding='post', maxlen=max_length) \n",
    "    x_test_3 = pad_sequences(x_test_3, padding='post', maxlen=max_length)\n",
    "       \n",
    "    x_train_4 = pad_sequences(x_train_4, padding='post', maxlen=max_length) \n",
    "    x_test_4 = pad_sequences(x_test_4, padding='post', maxlen=max_length)\n",
    "\n",
    "    # Fit the Transformer model\n",
    "    \n",
    "#     snap1 = tracemalloc.take_snapshot()\n",
    "    \n",
    "    mymodel = transformer_model_4_flatten(vocab_size, embedding_dim,max_length,dropout_rate)\n",
    "    mymodel.fit([x_train_1,x_train_2,x_train_3,x_train_4], y_train, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    # Collect and log evaluation metrics\n",
    "    auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(mymodel, [x_test_1,x_test_2,x_test_3,x_test_4], y_test)\n",
    "    model_time = time.time() - model_time_start\n",
    "\n",
    "    with open('other/soap_self_harm.csv','a') as fd:\n",
    "        fd.write(f'{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{model_time},{tn},{fp},{fn},{tp}\\n')\n",
    "\n",
    "    # Save predictions\n",
    "    path = 'other/predictions_pct_' + str(pct)+ '/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    pred_train = mymodel.predict([x_train_1,x_train_2,x_train_3,x_train_4])    \n",
    "    pred_train_10.append(pred_train)\n",
    "    np.save(path + 'pred_train_' + str(ii), pred_train)\n",
    "    \n",
    "    pred_test = mymodel.predict([x_test_1,x_test_2,x_test_3,x_test_4])\n",
    "    pred_test_10.append(pred_test)\n",
    "    np.save(path + 'pred_test_' + str(ii), pred_test)\n",
    "    \n",
    "    del mymodel,x_train_1,x_train_2,x_train_3,x_train_4,x_test_1,x_test_2,x_test_3,x_test_4\n",
    "    gc.collect()\n",
    "    \n",
    "    time_e = time.time() - time_s\n",
    "    with open('other/soap_self_harm.csv','a') as fd:\n",
    "        fd.write(f'1 iteration 18 DA,{time_e}\\n')\n",
    "\n",
    "running_time = time.time() - time_start\n",
    "with open('other/soap_self_harm.csv','a') as fd:\n",
    "        fd.write(f'10 iteration training time,{running_time}\\n')\n",
    "\n",
    "# Save Predicted Probabilities\n",
    "pd_pred_train_10 = pd.DataFrame(np.array(pred_train_10).reshape(np.array(pred_train_10).shape[0],np.array(pred_train_10).shape[1]))\n",
    "pd_pred_train_10 = pd_pred_train_10.transpose()\n",
    "\n",
    "pd_pred_test_10 = pd.DataFrame(np.array(pred_test_10).reshape(np.array(pred_test_10).shape[0],np.array(pred_test_10).shape[1]))\n",
    "pd_pred_test_10 = pd_pred_test_10.transpose()\n",
    "\n",
    "pd_under_idx_10 = pd.DataFrame(np.array(train_idx_under_10).reshape(np.array(train_idx_under_10).shape[0],np.array(train_idx_under_10).shape[1]))\n",
    "pd_under_idx_10 = pd_under_idx_10.transpose()\n",
    "\n",
    "pd_pred_train_10.to_csv(\"other2\\pd_pred_train_10_pct_\" + str(pct) + \".csv\")\n",
    "pd_pred_test_10.to_csv(\"other2\\pd_pred_test_10_pct_\" + str(pct) + \".csv\")\n",
    "pd_under_idx_10.to_csv(\"other3\\pd_under_idx_10_pct_\" + str(pct) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes Data alongside Structured Data with and without Under-Smapling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_str = pd.get_dummies(data[['Sex','Race','MaritalStatus','age','AB109']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_str['age'], bins=20)\n",
    "plt.title('Age Distribution')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data[data['event']==1]['age'], bins=20)\n",
    "plt.title('Age Distribution for Positives')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_group(x):\n",
    "    if x < 25:\n",
    "        group = 'lt_25'\n",
    "    elif x < 35:\n",
    "        group = '25-35'\n",
    "    elif x < 45:\n",
    "        group = '35-45'\n",
    "    elif x < 55:\n",
    "        group = '45-55'\n",
    "    else:\n",
    "        group = 'gt_55'\n",
    "    \n",
    "    return(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age_cat'] = [age_group(x) for x in data['age']] # group age\n",
    "data['AB109'] = [1 if x else 0 for x in data['AB109']] # convert true/false values to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1925/1925 [==============================] - 3137s 2s/step - loss: 0.0205 - auc: 0.6406\n",
      "Epoch 2/20\n",
      "1925/1925 [==============================] - 3003s 2s/step - loss: 0.0158 - auc: 0.7642\n",
      "Epoch 3/20\n",
      "1925/1925 [==============================] - 2960s 2s/step - loss: 0.0111 - auc: 0.8556\n",
      "Epoch 4/20\n",
      "1925/1925 [==============================] - 2975s 2s/step - loss: 0.0071 - auc: 0.9237\n",
      "Epoch 5/20\n",
      "1925/1925 [==============================] - 2964s 2s/step - loss: 0.0050 - auc: 0.9552\n",
      "Epoch 6/20\n",
      "1925/1925 [==============================] - 2957s 2s/step - loss: 0.0033 - auc: 0.9793\n",
      "Epoch 7/20\n",
      "1925/1925 [==============================] - 2961s 2s/step - loss: 0.0022 - auc: 0.9853\n",
      "Epoch 8/20\n",
      "1925/1925 [==============================] - 2953s 2s/step - loss: 0.0014 - auc: 0.9913\n",
      "Epoch 9/20\n",
      "1925/1925 [==============================] - 2968s 2s/step - loss: 0.0015 - auc: 0.9884\n",
      "Epoch 10/20\n",
      "1925/1925 [==============================] - 2963s 2s/step - loss: 0.0018 - auc: 0.9883\n",
      "Epoch 11/20\n",
      "1925/1925 [==============================] - 2971s 2s/step - loss: 0.0015 - auc: 0.9913\n",
      "Epoch 12/20\n",
      "1925/1925 [==============================] - 2960s 2s/step - loss: 0.0011 - auc: 0.9942\n",
      "Epoch 13/20\n",
      "1925/1925 [==============================] - 3008s 2s/step - loss: 0.0014 - auc: 0.9942\n",
      "Epoch 14/20\n",
      "1925/1925 [==============================] - 2956s 2s/step - loss: 0.0010 - auc: 0.9971\n",
      "Epoch 15/20\n",
      "1925/1925 [==============================] - 2965s 2s/step - loss: 8.4552e-04 - auc: 0.9942\n",
      "Epoch 16/20\n",
      "1925/1925 [==============================] - 2971s 2s/step - loss: 8.3925e-04 - auc: 0.9914\n",
      "Epoch 17/20\n",
      "1925/1925 [==============================] - 2973s 2s/step - loss: 9.6386e-04 - auc: 0.9913\n",
      "Epoch 18/20\n",
      "1925/1925 [==============================] - 3010s 2s/step - loss: 8.5623e-04 - auc: 0.9942\n",
      "Epoch 19/20\n",
      "1925/1925 [==============================] - 2987s 2s/step - loss: 8.2197e-04 - auc: 0.9914\n",
      "Epoch 20/20\n",
      "1925/1925 [==============================] - 2997s 2s/step - loss: 2.2851e-04 - auc: 0.9971\n",
      "Epoch 1/20\n",
      "1925/1925 [==============================] - 2972s 2s/step - loss: 0.0211 - auc: 0.6220\n",
      "Epoch 2/20\n",
      "1925/1925 [==============================] - 2974s 2s/step - loss: 0.0155 - auc: 0.7839\n",
      "Epoch 3/20\n",
      "1925/1925 [==============================] - 3026s 2s/step - loss: 0.0097 - auc: 0.9051\n",
      "Epoch 4/20\n",
      "1925/1925 [==============================] - 2950s 2s/step - loss: 0.0056 - auc: 0.9606\n",
      "Epoch 5/20\n",
      "1925/1925 [==============================] - 2958s 2s/step - loss: 0.0036 - auc: 0.9733\n",
      "Epoch 6/20\n",
      "1925/1925 [==============================] - 2943s 2s/step - loss: 0.0031 - auc: 0.9766\n",
      "Epoch 7/20\n",
      "1925/1925 [==============================] - 2964s 2s/step - loss: 0.0020 - auc: 0.9825\n",
      "Epoch 8/20\n",
      "1925/1925 [==============================] - 2950s 2s/step - loss: 0.0019 - auc: 0.9884\n",
      "Epoch 9/20\n",
      "1925/1925 [==============================] - 2973s 2s/step - loss: 0.0015 - auc: 0.9942\n",
      "Epoch 10/20\n",
      "1925/1925 [==============================] - 2945s 2s/step - loss: 0.0011 - auc: 0.9913\n",
      "Epoch 11/20\n",
      "1925/1925 [==============================] - 2962s 2s/step - loss: 8.0423e-04 - auc: 0.9971\n",
      "Epoch 12/20\n",
      "1925/1925 [==============================] - 2943s 2s/step - loss: 6.2510e-04 - auc: 0.9971\n",
      "Epoch 13/20\n",
      "1925/1925 [==============================] - 2954s 2s/step - loss: 4.5291e-04 - auc: 0.9971\n",
      "Epoch 14/20\n",
      "1925/1925 [==============================] - 2955s 2s/step - loss: 0.0012 - auc: 0.9913\n",
      "Epoch 15/20\n",
      "1925/1925 [==============================] - 2977s 2s/step - loss: 2.1327e-04 - auc: 0.9971\n",
      "Epoch 16/20\n",
      "1925/1925 [==============================] - 2990s 2s/step - loss: 3.4642e-04 - auc: 0.9971\n",
      "Epoch 17/20\n",
      "1925/1925 [==============================] - 2952s 2s/step - loss: 0.0012 - auc: 0.9885\n",
      "Epoch 18/20\n",
      "1925/1925 [==============================] - 2958s 2s/step - loss: 5.2567e-04 - auc: 1.0000\n",
      "Epoch 19/20\n",
      "1925/1925 [==============================] - 2947s 2s/step - loss: 3.0685e-04 - auc: 1.0000\n",
      "Epoch 20/20\n",
      "1925/1925 [==============================] - 2960s 2s/step - loss: 1.0426e-04 - auc: 1.0000\n",
      "Epoch 1/20\n",
      "1925/1925 [==============================] - 2964s 2s/step - loss: 0.0206 - auc: 0.6524\n",
      "Epoch 2/20\n",
      "1925/1925 [==============================] - 2987s 2s/step - loss: 0.0152 - auc: 0.8012\n",
      "Epoch 3/20\n",
      "1925/1925 [==============================] - 2983s 2s/step - loss: 0.0100 - auc: 0.8822\n",
      "Epoch 4/20\n",
      "1925/1925 [==============================] - 2973s 2s/step - loss: 0.0068 - auc: 0.9193\n",
      "Epoch 5/20\n",
      "1925/1925 [==============================] - 2972s 2s/step - loss: 0.0048 - auc: 0.9698\n",
      "Epoch 6/20\n",
      "1925/1925 [==============================] - 2966s 2s/step - loss: 0.0032 - auc: 0.9706\n",
      "Epoch 7/20\n",
      "1925/1925 [==============================] - 2972s 2s/step - loss: 0.0026 - auc: 0.9852\n",
      "Epoch 8/20\n",
      "1925/1925 [==============================] - 2978s 2s/step - loss: 0.0016 - auc: 0.9826\n",
      "Epoch 9/20\n",
      "1925/1925 [==============================] - 2981s 2s/step - loss: 0.0017 - auc: 0.9855\n",
      "Epoch 10/20\n",
      "1925/1925 [==============================] - 2984s 2s/step - loss: 0.0014 - auc: 0.9855\n",
      "Epoch 11/20\n",
      "1925/1925 [==============================] - 2990s 2s/step - loss: 0.0017 - auc: 0.9913\n",
      "Epoch 12/20\n",
      "1925/1925 [==============================] - 3015s 2s/step - loss: 9.7955e-04 - auc: 0.9942\n",
      "Epoch 13/20\n",
      "1925/1925 [==============================] - 2990s 2s/step - loss: 0.0011 - auc: 0.9942\n",
      "Epoch 14/20\n",
      "1925/1925 [==============================] - 2986s 2s/step - loss: 8.3830e-04 - auc: 0.9942\n",
      "Epoch 15/20\n",
      "1925/1925 [==============================] - 2965s 2s/step - loss: 0.0011 - auc: 0.9885\n",
      "Epoch 16/20\n",
      "1925/1925 [==============================] - 2990s 2s/step - loss: 5.3399e-04 - auc: 0.9971\n",
      "Epoch 17/20\n",
      "1925/1925 [==============================] - 2993s 2s/step - loss: 0.0011 - auc: 0.9942\n",
      "Epoch 18/20\n",
      "1925/1925 [==============================] - 3009s 2s/step - loss: 9.2209e-04 - auc: 0.9914\n",
      "Epoch 19/20\n",
      "1925/1925 [==============================] - 2996s 2s/step - loss: 8.3542e-04 - auc: 0.9971\n",
      "Epoch 20/20\n",
      "1925/1925 [==============================] - 2995s 2s/step - loss: 0.0011 - auc: 0.9914\n",
      "Epoch 1/20\n",
      "1925/1925 [==============================] - 2982s 2s/step - loss: 0.0209 - auc: 0.6147\n",
      "Epoch 2/20\n",
      "1925/1925 [==============================] - 2979s 2s/step - loss: 0.0150 - auc: 0.7949\n",
      "Epoch 3/20\n",
      "1925/1925 [==============================] - 2969s 2s/step - loss: 0.0094 - auc: 0.8863\n",
      "Epoch 4/20\n",
      "1925/1925 [==============================] - 2961s 2s/step - loss: 0.0065 - auc: 0.9274\n",
      "Epoch 5/20\n",
      "1925/1925 [==============================] - 2984s 2s/step - loss: 0.0050 - auc: 0.9754\n",
      "Epoch 6/20\n",
      "1925/1925 [==============================] - 2968s 2s/step - loss: 0.0028 - auc: 0.9737\n",
      "Epoch 7/20\n",
      "1925/1925 [==============================] - 2989s 2s/step - loss: 0.0027 - auc: 0.9824\n",
      "Epoch 8/20\n",
      "1925/1925 [==============================] - 2975s 2s/step - loss: 0.0015 - auc: 0.9884\n",
      "Epoch 9/20\n",
      "1925/1925 [==============================] - 2988s 2s/step - loss: 0.0014 - auc: 0.9884\n",
      "Epoch 10/20\n",
      "1925/1925 [==============================] - 2970s 2s/step - loss: 0.0013 - auc: 0.9970\n",
      "Epoch 11/20\n",
      "1925/1925 [==============================] - 3004s 2s/step - loss: 6.4009e-04 - auc: 0.9942\n",
      "Epoch 12/20\n",
      "1925/1925 [==============================] - 2970s 2s/step - loss: 0.0011 - auc: 0.9884\n",
      "Epoch 13/20\n",
      "1925/1925 [==============================] - 3017s 2s/step - loss: 0.0011 - auc: 0.9971\n",
      "Epoch 14/20\n",
      "1925/1925 [==============================] - 2979s 2s/step - loss: 9.0044e-04 - auc: 0.9913\n",
      "Epoch 15/20\n",
      "1925/1925 [==============================] - 2973s 2s/step - loss: 9.9182e-04 - auc: 0.9913\n",
      "Epoch 16/20\n",
      "1925/1925 [==============================] - 3001s 2s/step - loss: 6.6663e-04 - auc: 0.9971\n",
      "Epoch 17/20\n",
      "1925/1925 [==============================] - 3002s 2s/step - loss: 5.8872e-04 - auc: 0.9914\n",
      "Epoch 18/20\n",
      "1925/1925 [==============================] - 3019s 2s/step - loss: 0.0010 - auc: 0.9885\n",
      "Epoch 19/20\n",
      "1925/1925 [==============================] - 3021s 2s/step - loss: 0.0014 - auc: 0.9913\n",
      "Epoch 20/20\n",
      "1925/1925 [==============================] - 3048s 2s/step - loss: 0.0011 - auc: 0.9942\n"
     ]
    }
   ],
   "source": [
    "# No augmentation\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "max_length = 162 \n",
    "dropout_rate = 0.3\n",
    "embedding_dim = 200\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "time_1 = time.time()\n",
    "\n",
    "X = data['soap_obj']\n",
    "X_2 = data['soap_sub']\n",
    "X_3 = data['asses_plan']\n",
    "X_4 = data['quick_notes']\n",
    "X_5 = pd.get_dummies(data[['Sex','Race','MaritalStatus','age_cat','AB109']])\n",
    "y = data['event']\n",
    "\n",
    "j = 0 # to keep track of the iteration number\n",
    "time_start = time.time()\n",
    "\n",
    "#pct = 0.5 # Percent of positive cases after undersampling\n",
    "\n",
    "with open('other/soap_self_harm.csv','a') as fd:\n",
    "#     fd.write(f'No augmentation_4 heads_maxlen_160_replace_words_5-inputs_Structured_Alex_undersample_{pct}\\n')  \n",
    "    fd.write(f'No augmentation_4 heads_maxlen_160_replace_words_5-inputs_Structured_Alex_NO_undersampling\\n')\n",
    "    \n",
    "# Run the model 10 times with a different split each time  \n",
    "for ii in range(6,10):\n",
    "    time_s = time.time()\n",
    "    \n",
    "    j += 1\n",
    "    iteration = \"iter\" + str(j)\n",
    "    \n",
    "    # Train and test data\n",
    "    train_index = pd_train_idx_10.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "        \n",
    "    ##################################################################################    \n",
    "#     # Undersample the negative cases of the training set, no changes to test sets\n",
    "#     y_train_neg = y[train_index][y[train_index]==0]\n",
    "#     y_train_pos = y[train_index][y[train_index]==1]\n",
    "\n",
    "#     num_samples = round((1-pct)/pct*len(y_train_pos)) # Number of samples needed from the negatives\n",
    "#     samples = y_train_neg.sample(n=num_samples, replace=False, random_state=0) # Downsample training set\n",
    "    \n",
    "#     y_train_neg = y_train_neg[samples.index]\n",
    "#     train_idx_under = pd.concat([y_train_neg, y_train_pos], axis=0).index # Indices for undersmpled training set\n",
    "            \n",
    "#     x_train_1, x_test_1 = X[train_idx_under], X[test_index]\n",
    "#     x_train_2, x_test_2 = X_2[train_idx_under], X_2[test_index]\n",
    "#     x_train_3, x_test_3 = X_3[train_idx_under], X_3[test_index]\n",
    "#     x_train_4, x_test_4 = X_4[train_idx_under], X_4[test_index]\n",
    "#     x_train_5, x_test_5 = X_5.iloc[train_idx_under,:], X_5.iloc[test_index,:]\n",
    "#     y_train, y_test = y[train_idx_under], y[test_index]\n",
    "    ##################################################################################\n",
    "    # The following is for no undersampling\n",
    "    x_train_1, x_test_1 = X[train_index], X[test_index]\n",
    "    x_train_2, x_test_2 = X_2[train_index], X_2[test_index]\n",
    "    x_train_3, x_test_3 = X_3[train_index], X_3[test_index]\n",
    "    x_train_4, x_test_4 = X_4[train_index], X_4[test_index]\n",
    "    x_train_5, x_test_5 = X_5.iloc[train_index,:], X_5.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    ##################################################################################\n",
    "\n",
    "    # Run CNN model\n",
    "    model_time_start = time.time()\n",
    "\n",
    "\n",
    "    # Tokenize the text   \n",
    "    tokenizer = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer.fit_on_texts(x_train_1)\n",
    "    x_train_1 = tokenizer.texts_to_sequences(x_train_1)\n",
    "    x_test_1 = tokenizer.texts_to_sequences(x_test_1)\n",
    "    vocab_size = len(tokenizer.word_index) + 1 # plus the reserved index 0\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    tokenizer_2 = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer_2.fit_on_texts(x_train_2)\n",
    "    x_train_2 = tokenizer_2.texts_to_sequences(x_train_2)\n",
    "    x_test_2 = tokenizer_2.texts_to_sequences(x_test_2)\n",
    "    \n",
    "    tokenizer_3 = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer_3.fit_on_texts(x_train_3)\n",
    "    x_train_3 = tokenizer_3.texts_to_sequences(x_train_3)\n",
    "    x_test_3 = tokenizer_3.texts_to_sequences(x_test_3)\n",
    "     \n",
    "    tokenizer_4 = Tokenizer(num_words=5000) # get the frequency of all tokens and use the 5000 most common ones\n",
    "    tokenizer_4.fit_on_texts(x_train_4)\n",
    "    x_train_4 = tokenizer_4.texts_to_sequences(x_train_4)\n",
    "    x_test_4 = tokenizer_4.texts_to_sequences(x_test_4)\n",
    "\n",
    "    # Pad the sequences with 0's\n",
    "    x_train_1 = pad_sequences(x_train_1, padding='post', maxlen=max_length) \n",
    "    x_test_1 = pad_sequences(x_test_1, padding='post', maxlen=max_length)\n",
    "    \n",
    "    x_train_2 = pad_sequences(x_train_2, padding='post', maxlen=max_length) \n",
    "    x_test_2 = pad_sequences(x_test_2, padding='post', maxlen=max_length)\n",
    "    \n",
    "    x_train_3 = pad_sequences(x_train_3, padding='post', maxlen=max_length) \n",
    "    x_test_3 = pad_sequences(x_test_3, padding='post', maxlen=max_length)\n",
    "    \n",
    "    x_train_4 = pad_sequences(x_train_4, padding='post', maxlen=max_length) \n",
    "    x_test_4 = pad_sequences(x_test_4, padding='post', maxlen=max_length)\n",
    "\n",
    "    # Fit the Transformer model\n",
    "    mymodel = transformer_model_5_structured(vocab_size, embedding_dim,max_length,dropout_rate)\n",
    "    mymodel.fit([x_train_1,x_train_2,x_train_3,x_train_4,x_train_5], y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Collect and log evaluation metrics\n",
    "    auc_roc, auc_pr, acc, precision, recall, specificity,f1,tn,fp,fn,tp = evaluate(mymodel, [x_test_1,x_test_2,x_test_3,x_test_4,x_test_5], y_test)\n",
    "    model_time = time.time() - model_time_start\n",
    "\n",
    "    with open('other/soap_self_harm.csv','a') as fd:\n",
    "        fd.write(f'{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp}\\n')\n",
    "\n",
    "    del mymodel, tokenizer, tokenizer_2, tokenizer_3, tokenizer_4 \n",
    "    gc.collect()\n",
    "    \n",
    "    time_e = time.time() - time_s\n",
    "    with open('other/soap_self_harm.csv','a') as fd:\n",
    "        fd.write(f'1 iteration 18 DA,{time_e}\\n')\n",
    "\n",
    "running_time = time.time() - time_start\n",
    "with open('other/soap_self_harm.csv','a') as fd:\n",
    "        fd.write(f'10 iteration training time,{running_time}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
