{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suicidal and Self-Injurious Incidents Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This script runs Machine Learning models on structured data alone and on structured data alongside predicted probabiliites from the Transformer Encoder model on ntoes data alone, with and without under-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For handeling dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import re                                  # For regular expression operations\n",
    "import string                              # For string operations\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "# For building neural netwrok models\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.layers import Dense, Flatten, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87975, 18)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"Z:/rscripts/users/interns/hlu/data/cleaned_model_str_notes_Alex.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'InmateID', 'BookingNumber', 'DateOfBirth', 'Sex', 'Race',\n",
       "       'MaritalStatus', 'BookingDate', 'LastUpdateDateTime', 'ReleaseDate',\n",
       "       'soap_sub', 'soap_obj', 'soap_ass', 'soap_plan', 'quick_notes', 'event',\n",
       "       'age', 'AB109'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the useless column\n",
    "structured = data[['Sex', 'Race', 'MaritalStatus', 'age', 'AB109', 'event']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured['AB109'] = [1 if x==\"Ture\" else 0 for x in structured['AB109']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_group(x):\n",
    "    if x < 25:\n",
    "        group = 'lt_25'\n",
    "    elif x < 35:\n",
    "        group = '25-35'\n",
    "    elif x < 45:\n",
    "        group = '35-45'\n",
    "    elif x < 55:\n",
    "        group = '45-55'\n",
    "    else:\n",
    "        group = 'gt_55'\n",
    "    \n",
    "    return(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured['age_cat'] = [age_group(x) for x in structured['age']] # group age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(structured[['Sex','Race','MaritalStatus','age_cat','AB109']])\n",
    "y = structured['event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    pred = (y_pred > 0.5).astype(\"int32\")\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, pred).ravel()\n",
    "\n",
    "    precision = tp/(tp + fp)\n",
    "    recall = tp/(tp + fn)\n",
    "    specificity = tn/(tn + fp)\n",
    "    f1 = (2*precision*recall)/(precision + recall)\n",
    "    acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "    \n",
    "    auc_roc = round(roc_auc_score(y, y_pred),4)\n",
    "    pre, rec, thresholds = precision_recall_curve(y, y_pred)\n",
    "    auc_pr = round(auc(rec, pre),4)\n",
    "    return(auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load indicies and predictions from notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 10 training and test sets\n",
    "pd_train_idx_10 = pd.read_csv('other/pd_train_idx_10.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv')\n",
    "\n",
    "pd_train_idx_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21382</td>\n",
       "      <td>67168</td>\n",
       "      <td>44241</td>\n",
       "      <td>32895</td>\n",
       "      <td>50243</td>\n",
       "      <td>16907</td>\n",
       "      <td>59817</td>\n",
       "      <td>84579</td>\n",
       "      <td>42076</td>\n",
       "      <td>26331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9973</td>\n",
       "      <td>21571</td>\n",
       "      <td>52839</td>\n",
       "      <td>8788</td>\n",
       "      <td>67684</td>\n",
       "      <td>29912</td>\n",
       "      <td>11357</td>\n",
       "      <td>46855</td>\n",
       "      <td>26480</td>\n",
       "      <td>55932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9\n",
       "0  21382  67168  44241  32895  50243  16907  59817  84579  42076  26331\n",
       "1   9973  21571  52839   8788  67684  29912  11357  46855  26480  55932"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train_idx_10.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions from text data\n",
    "\n",
    "pd_pred_train_10 = pd.read_csv(\"other2\\pd_pred_train_10.csv\")\n",
    "pd_pred_test_10 = pd.read_csv(\"other2\\pd_pred_test_10.csv\")\n",
    "\n",
    "pd_pred_train_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_pred_test_10.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pd_pred_train_10.iloc[:,4].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lg = LogisticRegression(penalty = 'l1', solver = 'liblinear', C=5, l1_ratio =0)\n",
    "model_rf = RandomForestClassifier(min_samples_split = 5, criterion = 'entropy', max_depth = 10,max_features='auto')\n",
    "model_xgboost = XGBClassifier(verbosity = 0, booster = 'gblinear', max_depth = 4, n_estimators = 200)\n",
    "model_knn = KNeighborsClassifier(n_neighbors = 5, weights= 'distance')\n",
    "model_svm = SVC(kernel=\"rbf\", C=0.1, gamma=0.001, probability=True)\n",
    "\n",
    "models = [model_lg, model_rf, model_xgboost, model_knn, model_svm]\n",
    "model_names = ['model_lg', 'model_rf', 'model_xgboost', 'model_knn', 'model_svm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the log of the predicted probabilities to avoid near 0 values\n",
    "def log_data(x):\n",
    "    if np.log(x) < -100:\n",
    "        log_x = -100\n",
    "    else:\n",
    "        log_x = np.log(x)\n",
    "    \n",
    "    return(log_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Undersampling Without Predictions from Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n",
      "iter10 ....\n"
     ]
    }
   ],
   "source": [
    "with open('structured_textpred/structured_only.csv','a') as fd:\n",
    "    fd.write(f'Structured data only + no understampling + Save Predictions for Ensemble\\n')\n",
    "\n",
    "\n",
    "pred_train_10 = []\n",
    "pred_test_10 = []\n",
    "    \n",
    "j = 0 # to keep track of the iteration number    \n",
    "for ii in range(10):\n",
    "    time_s = time.time()\n",
    "    \n",
    "    # Train and test data\n",
    "    train_index = pd_train_idx_10.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    j += 1\n",
    "    iteration = \"iter\" + str(j)\n",
    "    print(iteration, '....')\n",
    "    x_train, x_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "     # Run models\n",
    "    for jj in range(len(models)):\n",
    "        model_time_start = time.time()\n",
    "        model = models[jj]\n",
    "        model.fit(x_train, y_train)\n",
    "         # Collect and log evaluation metrics\n",
    "        auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "        model_time = time.time() - model_time_start\n",
    "        \n",
    "        \n",
    "         # Save predictions\n",
    "        pred_train = model.predict_proba(x_train)    \n",
    "        pred_train_10.append(pred_train[:,1])\n",
    "        np.save('other/predictions_ML/' + 'pred_train_pct28_noText' + str(model_names[jj]), pred_train)\n",
    "        \n",
    "        pred_test = model.predict_proba(x_test)\n",
    "        pred_test_10.append(pred_test[:,1])\n",
    "        np.save('other/predictions_ML/' + 'pred_train_pct28_noText' + str(model_names[jj]), pred_test)\n",
    "\n",
    "        with open('structured_textpred/structured_only.csv','a') as fd:\n",
    "            fd.write(f'{model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n",
    "\n",
    "# Save predicted probabilities for ensemble models\n",
    "pd_pred_train_10 = pd.DataFrame(np.array(pred_train_10).reshape(np.array(pred_train_10).shape[0],np.array(pred_train_10).shape[1]))\n",
    "pd_pred_train_10 = pd_pred_train_10.transpose()\n",
    "\n",
    "pd_pred_test_10 = pd.DataFrame(np.array(pred_test_10).reshape(np.array(pred_test_10).shape[0],np.array(pred_test_10).shape[1]))\n",
    "pd_pred_test_10 = pd_pred_test_10.transpose()\n",
    "\n",
    "pd_pred_train_10.to_csv(\"other2\\pd_pred_train_10_ML_noText_pct28.csv\") # save predictions from machine learning models\n",
    "pd_pred_test_10.to_csv(\"other2\\pd_pred_test_10_ML_noText_pct28.csv\") # save predictions from machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling without Predictions from Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "# Undersampling to pct prevalence\n",
    "pct = 0.1\n",
    "\n",
    "# Load the 10 undersampled training sets\n",
    "pd_train_idx_10pct = pd.read_csv(f'other3/pd_under_idx_10_pct_{pct}.csv')\n",
    "\n",
    "pred_train_10 = []\n",
    "pred_test_10 = []\n",
    "\n",
    "with open('structured_textpred/structured_only.csv','a') as fd:\n",
    "    fd.write(f'Undersampling Structured Data only to pct_{pct} prevalence + Save predictions for Ensemble Learner\\n')\n",
    "    \n",
    "    \n",
    "for ii in range(10):\n",
    "    time_s = time.time()\n",
    "    \n",
    "    # Train and test data\n",
    "    train_index = pd_train_idx_10pct.iloc[:, ii].values # Undersampled training sets\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values # Test sets remain the same\n",
    "    \n",
    "\n",
    "    iteration = \"iter\" + str(ii) # to keep track of the iteration number \n",
    "    print(iteration, '....')\n",
    "    x_train, x_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "     # Run models\n",
    "    for jj in range(len(models)):\n",
    "        model_time_start = time.time()\n",
    "        model = models[jj]\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "         # Collect and log evaluation metrics\n",
    "        auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "        model_time = time.time() - model_time_start\n",
    "\n",
    "        with open('structured_textpred/structured_only.csv','a') as fd:\n",
    "            fd.write(f'{model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n",
    "\n",
    "        # Save predictions\n",
    "        pred_train = model.predict_proba(x_train)    \n",
    "        pred_train_10.append(pred_train[:,1])\n",
    "        np.save('other/predictions_ML/' + f'pred_train_pct{pct}_noText' + str(model_names[jj]), pred_train)\n",
    "        \n",
    "        pred_test = model.predict_proba(x_test)\n",
    "        pred_test_10.append(pred_test[:,1])\n",
    "        np.save(f'other/predictions_ML/' + f'pred_train_pct{pct}_noText' + str(model_names[jj]), pred_test)\n",
    "        \n",
    "# Save predicted probabilities for ensemble models\n",
    "pd_pred_train_10 = pd.DataFrame(np.array(pred_train_10).reshape(np.array(pred_train_10).shape[0],np.array(pred_train_10).shape[1]))\n",
    "pd_pred_train_10 = pd_pred_train_10.transpose()\n",
    "\n",
    "pd_pred_test_10 = pd.DataFrame(np.array(pred_test_10).reshape(np.array(pred_test_10).shape[0],np.array(pred_test_10).shape[1]))\n",
    "pd_pred_test_10 = pd_pred_test_10.transpose()\n",
    "\n",
    "pd_pred_train_10.to_csv(f\"other2\\pd_pred_train_10_ML_noText_pct{pct}.csv\") # save predictions from machine learning models\n",
    "pd_pred_test_10.to_csv(f\"other2\\pd_pred_test_10_ML_noText_pct{pct}.csv\") # save predictions from machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No undersampling + Predictions from Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-33.38693239],\n",
       "       [-28.42784878],\n",
       "       [-30.98754316],\n",
       "       ...,\n",
       "       [-22.89752773],\n",
       "       [-33.81137466],\n",
       "       [-31.31847383]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(x_train['pred_text']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n",
      "iter10 ....\n"
     ]
    }
   ],
   "source": [
    "# Load predictions from text data\n",
    "\n",
    "pd_pred_train_10 = pd.read_csv(\"other2\\pd_pred_train_10.csv\")\n",
    "pd_pred_test_10 = pd.read_csv(\"other2\\pd_pred_test_10.csv\")\n",
    "\n",
    "pd_pred_train_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_pred_test_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'Structured data + predictions from text data + no understampling + save predictions for ensemble\\n')\n",
    "    \n",
    "\n",
    "pred_train_10 = []\n",
    "pred_test_10 = []\n",
    "\n",
    "\n",
    "j = 0 # to keep track of the iteration number    \n",
    "for ii in range(10):\n",
    "    time_s = time.time()\n",
    "    \n",
    "    # Train and test data\n",
    "    train_index = pd_train_idx_10.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    j += 1\n",
    "    iteration = \"iter\" + str(j)\n",
    "    print(iteration, '....')\n",
    "    x_train, x_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Predictions from text data (apply log on predictions from text to avoid numbers too small)\n",
    "    x_train['pred_text'] = [log_data(e) for e in pd_pred_train_10.iloc[:, ii]]\n",
    "    x_test['pred_text'] = [log_data(e) for e in pd_pred_test_10.iloc[:, ii]]\n",
    "    \n",
    "    # Scale pred_text\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.asarray(x_train['pred_text']).reshape(-1,1))\n",
    "    x_train['pred_text'] = scaler.transform(np.asarray(x_train['pred_text']).reshape(-1,1))\n",
    "    x_test['pred_text'] = scaler.transform(np.asarray(x_test['pred_text']).reshape(-1,1))\n",
    "    \n",
    "     # Run models\n",
    "    for jj in range(len(models)):\n",
    "        model_time_start = time.time()\n",
    "        model = models[jj]\n",
    "        model.fit(x_train, y_train)\n",
    "         # Collect and log evaluation metrics\n",
    "        auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "        model_time = time.time() - model_time_start\n",
    "        \n",
    "        \n",
    "        # Save predictions\n",
    "        pred_train = model.predict_proba(x_train)    \n",
    "        pred_train_10.append(pred_train[:,1])\n",
    "        np.save('other/predictions_ML/' + 'pred_train_Text_pct28' + str(model_names[jj]), pred_train)\n",
    "        \n",
    "        pred_test = model.predict_proba(x_test)\n",
    "        pred_test_10.append(pred_test[:,1])\n",
    "        np.save('other/predictions_ML/' + 'pred_test_Text_pct28' + str(model_names[jj]), pred_test)\n",
    "\n",
    "        with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "            fd.write(f'{model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n",
    "\n",
    "# Save predicted probabilities for ensemble models\n",
    "pd_pred_train_10 = pd.DataFrame(np.array(pred_train_10).reshape(np.array(pred_train_10).shape[0],np.array(pred_train_10).shape[1]))\n",
    "pd_pred_train_10 = pd_pred_train_10.transpose()\n",
    "\n",
    "pd_pred_test_10 = pd.DataFrame(np.array(pred_test_10).reshape(np.array(pred_test_10).shape[0],np.array(pred_test_10).shape[1]))\n",
    "pd_pred_test_10 = pd_pred_test_10.transpose()\n",
    "\n",
    "pd_pred_train_10.to_csv(\"other2\\pd_pred_train_10_ML_Text_pct28.csv\") # save predictions from machine learning models\n",
    "pd_pred_test_10.to_csv(\"other2\\pd_pred_test_10_ML_Text_pct28.csv\") # save predictions from machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling + Predictions from Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "# Undersampling to pct prevalence\n",
    "pct = 0.5\n",
    "\n",
    "# Load the indices of the 10 undersampled training sets\n",
    "pd_train_idx_pct = pd.read_csv(f'other3/pd_under_idx_10_pct_{pct}.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv') # Test sets remain the same\n",
    "\n",
    "# Load the predictions \n",
    "pd_train_pred_pct = pd.read_csv(f'other2/pd_pred_train_10_pct_{pct}.csv')\n",
    "pd_test_pred_pct = pd.read_csv(f'other2/pd_pred_test_10_pct_{pct}.csv')\n",
    "\n",
    "\n",
    "pd_train_idx_pct.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_train_pred_pct.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "pd_test_pred_pct.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "\n",
    "pred_train_10 = []\n",
    "pred_test_10 = []\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'Undersampling with Text Predictions to {pct} prevalence + save predictions for Ensemble Learner\\n')\n",
    "\n",
    "\n",
    "for ii in range(10):\n",
    "    time_s = time.time()\n",
    "    \n",
    "    # Train and test data\n",
    "    train_index = pd_train_idx_pct.iloc[:, ii].values # Undersampled training sets\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values # Test sets remain the same\n",
    "    \n",
    "    iteration = \"iter\" + str(ii) # to keep track of the iteration number    \n",
    "    print(iteration, '....')\n",
    "    x_train, x_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Predictions from text data (apply log on predictions from text to avoid numbers too small)\n",
    "    x_train['pred_text'] = [log_data(e) for e in pd_train_pred_pct.iloc[:, ii]]\n",
    "    x_test['pred_text'] = [log_data(e) for e in pd_test_pred_pct.iloc[:, ii]]\n",
    "    \n",
    "    # Scale pred_text\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.asarray(x_train['pred_text']).reshape(-1,1))\n",
    "    x_train['pred_text'] = scaler.transform(np.asarray(x_train['pred_text']).reshape(-1,1))\n",
    "    x_test['pred_text'] = scaler.transform(np.asarray(x_test['pred_text']).reshape(-1,1))\n",
    "\n",
    "    \n",
    "     # Run models\n",
    "    for jj in range(len(models)):\n",
    "        model_time_start = time.time()\n",
    "        model = models[jj]\n",
    "        model.fit(x_train, y_train)\n",
    "         # Collect and log evaluation metrics\n",
    "        auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "        model_time = time.time() - model_time_start\n",
    "\n",
    "        with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "            fd.write(f'{model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n",
    "\n",
    "        # Save predictions\n",
    "        pred_train = model.predict_proba(x_train)    \n",
    "        pred_train_10.append(pred_train[:,1])\n",
    "        np.save('other/predictions_ML/' + f'pred_train_pct{pct}_Text' + str(model_names[jj]), pred_train)\n",
    "        \n",
    "        pred_test = model.predict_proba(x_test)\n",
    "        pred_test_10.append(pred_test[:,1])\n",
    "        np.save(f'other/predictions_ML/' + f'pred_train_pct{pct}_Text' + str(model_names[jj]), pred_test)\n",
    "        \n",
    "# Save predicfted probabilities for ensemble models\n",
    "pd_pred_train_10 = pd.DataFrame(np.array(pred_train_10).reshape(np.array(pred_train_10).shape[0],np.array(pred_train_10).shape[1]))\n",
    "pd_pred_train_10 = pd_pred_train_10.transpose()\n",
    "\n",
    "pd_pred_test_10 = pd.DataFrame(np.array(pred_test_10).reshape(np.array(pred_test_10).shape[0],np.array(pred_test_10).shape[1]))\n",
    "pd_pred_test_10 = pd_pred_test_10.transpose()\n",
    "\n",
    "pd_pred_train_10.to_csv(f\"other2\\pd_pred_train_10_ML_Text_pct{pct}.csv\") # save predictions from machine learning models\n",
    "pd_pred_test_10.to_csv(f\"other2\\pd_pred_test_10_ML_Text_pct{pct}.csv\") # save predictions from machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No_undersampling + Predictions from Notes for Ensemble Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "# Load the indices of the 10 training and test sets\n",
    "pd_train_idx_10 = pd.read_csv('other/pd_train_idx_10.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv') # Test sets remain the same\n",
    "\n",
    "pd_train_idx_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True) \n",
    "\n",
    "# Load predictions from Machine Learning Models with text data (No Undersampling)\n",
    "\n",
    "ML_pred_train_10 = pd.read_csv(\"other2\\pd_pred_train_10_ML_Text_pct28.csv\")\n",
    "ML_pred_test_10 = pd.read_csv(\"other2\\pd_pred_test_10_ML_Text_pct28.csv\")\n",
    "\n",
    "ML_pred_train_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "ML_pred_test_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'No_undersampling + Text Prediction: Ensemble Learner model performance + remove SVM + multiple_models\\n')\n",
    "\n",
    "for ii in range(10):\n",
    "    \n",
    "    train_index = pd_train_idx_10.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    iteration = \"iter\" + str(ii)\n",
    "    print(iteration, '....')\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Take the predictions from the 5 models\n",
    "    x_train = ML_pred_train_10.iloc[:,ii*5+0:ii*5+4]   \n",
    "    x_test = ML_pred_test_10.iloc[:,ii*5+0:ii*5+4]\n",
    "    \n",
    "     # Run models\n",
    "    for jj in range(len(models)):\n",
    "        model_time_start = time.time()\n",
    "        model = models[jj]\n",
    "        model.fit(x_train, y_train)\n",
    "         # Collect and log evaluation metrics\n",
    "        auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "        model_time = time.time() - model_time_start\n",
    "\n",
    "        with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "            fd.write(f'Ensemble, {model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling + Predictions from Notes for Ensemble Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "pct = 0.5\n",
    "\n",
    "# Load the indices of the 10 undersampled sets\n",
    "pd_train_idx_pct = pd.read_csv(f'other3/pd_under_idx_10_pct_{pct}.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv') # Test sets remain the same\n",
    "\n",
    "pd_train_idx_pct.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True) \n",
    "\n",
    "# Load the predictions from the 5 models\n",
    "ML_pred_train_10 = pd.read_csv(f\"other2\\pd_pred_train_10_ML_Text_pct{pct}.csv\")\n",
    "ML_pred_test_10 = pd.read_csv(f\"other2\\pd_pred_test_10_ML_Text_pct{pct}.csv\")\n",
    "\n",
    "ML_pred_train_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "ML_pred_test_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'Undersample to prevalence {pct} + Text Prediction + Ensemble Learner + remove SVM + multiple models\\n')\n",
    "\n",
    "for ii in range(10):\n",
    "    \n",
    "    train_index = pd_train_idx_pct.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    iteration = \"iter\" + str(ii)\n",
    "    print(iteration, '....')\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Take the predictions from the 5 models\n",
    "    x_train = ML_pred_train_10.iloc[:,ii*5+0:ii*5+4]   \n",
    "    x_test = ML_pred_test_10.iloc[:,ii*5+0:ii*5+4]\n",
    "    \n",
    "     # Run models\n",
    "    for jj in range(len(models)):\n",
    "        model_time_start = time.time()\n",
    "        model = models[jj]\n",
    "        model.fit(x_train, y_train)\n",
    "         # Collect and log evaluation metrics\n",
    "        auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "        model_time = time.time() - model_time_start\n",
    "\n",
    "        with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "            fd.write(f'Ensemble, {model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Under-sampling + No Predictions from Notes + Ensemble Learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "# Load the indices of the 10 training and test sets\n",
    "pd_train_idx_10 = pd.read_csv('other/pd_train_idx_10.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv') # Test sets remain the same\n",
    "\n",
    "pd_train_idx_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True) \n",
    "\n",
    "# Load the predictions from the 5 models\n",
    "ML_pred_train_10 = pd.read_csv(f\"other2\\pd_pred_train_10_ML_noText_pct28.csv\")\n",
    "ML_pred_test_10 = pd.read_csv(f\"other2\\pd_pred_test_10_ML_noText_pct28.csv\")\n",
    "\n",
    "ML_pred_train_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "ML_pred_test_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'No Undersampling + No Text Prediction + Ensemble Learner + remove SVM + multiple models\\n')\n",
    "\n",
    "for ii in range(10):\n",
    "    \n",
    "    train_index = pd_train_idx_10.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    iteration = \"iter\" + str(ii)\n",
    "    print(iteration, '....')\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Take the predictions from the 5 models\n",
    "    x_train = ML_pred_train_10.iloc[:,ii*5+0:ii*5+4]   \n",
    "    x_test = ML_pred_test_10.iloc[:,ii*5+0:ii*5+4]\n",
    "    \n",
    "     # Run models\n",
    "    for jj in range(len(models)):\n",
    "        model_time_start = time.time()\n",
    "        model = models[jj]\n",
    "        model.fit(x_train, y_train)\n",
    "         # Collect and log evaluation metrics\n",
    "        auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "        model_time = time.time() - model_time_start\n",
    "\n",
    "        with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "            fd.write(f'Ensemble, {model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under-sampling + No Predictions from Notes + Ensemble Learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "pct = 0.5\n",
    "\n",
    "# Load the indices of the 10 undersampled sets\n",
    "pd_train_idx_pct = pd.read_csv(f'other3/pd_under_idx_10_pct_{pct}.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv') # Test sets remain the same\n",
    "\n",
    "pd_train_idx_pct.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True) \n",
    "\n",
    "# Load the predictions from the 5 models\n",
    "ML_pred_train_10 = pd.read_csv(f\"other2\\pd_pred_train_10_ML_noText_pct{pct}.csv\")\n",
    "ML_pred_test_10 = pd.read_csv(f\"other2\\pd_pred_test_10_ML_noText_pct{pct}.csv\")\n",
    "\n",
    "ML_pred_train_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "ML_pred_test_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'Undersample to prevalence {pct} + No Text Prediction: Ensemble Learner + remove SVM + multiple models\\n')\n",
    "\n",
    "for ii in range(10):\n",
    "    \n",
    "    train_index = pd_train_idx_pct.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    iteration = \"iter\" + str(ii)\n",
    "    print(iteration, '....')\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Take the predictions from the 5 models\n",
    "    x_train = ML_pred_train_10.iloc[:,ii*5+0:ii*5+4]   \n",
    "    x_test = ML_pred_test_10.iloc[:,ii*5+0:ii*5+4]\n",
    "    \n",
    "     # Run models\n",
    "    for jj in range(len(models)):\n",
    "        model_time_start = time.time()\n",
    "        model = models[jj]\n",
    "        model.fit(x_train, y_train)\n",
    "         # Collect and log evaluation metrics\n",
    "        auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "        model_time = time.time() - model_time_start\n",
    "\n",
    "        with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "            fd.write(f'Ensemble, {model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Logistic with two-way Interactions (Structured Only + No Under-sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import scipy.special\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "# Load the indices of the 10 training and test sets\n",
    "pd_train_idx_10 = pd.read_csv('other/pd_train_idx_10.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv') # Test sets remain the same\n",
    "\n",
    "pd_train_idx_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True) \n",
    "\n",
    "# Load the predictions from the 5 models\n",
    "ML_pred_train_10 = pd.read_csv(f\"other2\\pd_pred_train_10_ML_noText_pct28.csv\")\n",
    "ML_pred_test_10 = pd.read_csv(f\"other2\\pd_pred_test_10_ML_noText_pct28.csv\")\n",
    "\n",
    "ML_pred_train_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "ML_pred_test_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'No Undersampling + No Text Prediction + Ensemble Learner + remove SVM + 2-way Interactions+LASSO\\n')\n",
    "\n",
    "for ii in range(10):\n",
    "    \n",
    "    train_index = pd_train_idx_10.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    iteration = \"iter\" + str(ii)\n",
    "    print(iteration, '....')\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Take the predictions from the 5 models\n",
    "    x_train = ML_pred_train_10.iloc[:,ii*5+0:ii*5+4]   \n",
    "    x_test = ML_pred_test_10.iloc[:,ii*5+0:ii*5+4]\n",
    "    \n",
    "    \n",
    "    # Scale predicted probabilites from Machine Learning models\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    # Two-way interaction\n",
    "    poly = PolynomialFeatures(interaction_only=True) # Only include interactions, no squares\n",
    "    x_train = poly.fit_transform(x_train)\n",
    "    x_test = poly.transform(x_test)\n",
    "    \n",
    "    \n",
    "     # Run models\n",
    "\n",
    "    model_time_start = time.time()\n",
    "#     model = LogisticRegression()\n",
    "    model = LogisticRegression(penalty = 'l1', solver = 'liblinear', C=5, l1_ratio =0)\n",
    "    model.fit(x_train, y_train)\n",
    "     # Collect and log evaluation metrics\n",
    "    auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "    model_time = time.time() - model_time_start\n",
    "\n",
    "    with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "        fd.write(f'Ensemble, {model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learner with 2-way Interaction (Stuctured Only + Under-sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "pct = 0.5\n",
    "\n",
    "# Load the indices of the 10 undersampled sets\n",
    "pd_train_idx_pct = pd.read_csv(f'other3/pd_under_idx_10_pct_{pct}.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv') # Test sets remain the same\n",
    "\n",
    "pd_train_idx_pct.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True) \n",
    "\n",
    "# Load the predictions from the 5 models\n",
    "ML_pred_train_10 = pd.read_csv(f\"other2\\pd_pred_train_10_ML_noText_pct{pct}.csv\")\n",
    "ML_pred_test_10 = pd.read_csv(f\"other2\\pd_pred_test_10_ML_noText_pct{pct}.csv\")\n",
    "\n",
    "ML_pred_train_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "ML_pred_test_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'Undersample to {pct} + No Text Prediction + Ensemble Learner + remove SVM + Interactions + LASSO\\n')\n",
    "\n",
    "for ii in range(10):\n",
    "    \n",
    "    train_index = pd_train_idx_pct.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    iteration = \"iter\" + str(ii)\n",
    "    print(iteration, '....')\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Take the predictions from the 5 models\n",
    "    x_train = ML_pred_train_10.iloc[:,ii*5+0:ii*5+4]   \n",
    "    x_test = ML_pred_test_10.iloc[:,ii*5+0:ii*5+4]\n",
    "       \n",
    "    # Scale predicted probabilites from Machine Learning models\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    # Two-way interaction\n",
    "    poly = PolynomialFeatures(interaction_only=True) # Only include interactions, no squares\n",
    "    x_train = poly.fit_transform(x_train)\n",
    "    x_test = poly.transform(x_test)\n",
    "        \n",
    "     # Run models\n",
    "    model_time_start = time.time()\n",
    "#     model = LogisticRegression()\n",
    "    model = LogisticRegression(penalty = 'l1', solver = 'liblinear', C=5, l1_ratio =0)\n",
    "    model.fit(x_train, y_train)\n",
    "     # Collect and log evaluation metrics\n",
    "    auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "    model_time = time.time() - model_time_start\n",
    "\n",
    "    with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "        fd.write(f'Ensemble, {model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learner with 2-way Interactions (Structured+Pred + No-undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "# Load the indices of the 10 training and test sets\n",
    "pd_train_idx_10 = pd.read_csv('other/pd_train_idx_10.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv') # Test sets remain the same\n",
    "\n",
    "pd_train_idx_10.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True) \n",
    "\n",
    "# Load predictions from Machine Learning Models with text data (No Undersampling)\n",
    "\n",
    "ML_pred_train_10 = pd.read_csv(\"other2\\pd_pred_train_10_ML_Text_pct28.csv\")\n",
    "ML_pred_test_10 = pd.read_csv(\"other2\\pd_pred_test_10_ML_Text_pct28.csv\")\n",
    "\n",
    "ML_pred_train_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "ML_pred_test_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'No_undersampling + Text Prediction: Ensemble Learner + remove SVM + Interactions + LASSO\\n')\n",
    "\n",
    "for ii in range(10):\n",
    "    \n",
    "    train_index = pd_train_idx_10.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    iteration = \"iter\" + str(ii)\n",
    "    print(iteration, '....')\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Take the predictions from the 5 models\n",
    "    x_train = ML_pred_train_10.iloc[:,ii*5+0:ii*5+4]   \n",
    "    x_test = ML_pred_test_10.iloc[:,ii*5+0:ii*5+4]\n",
    "    \n",
    "    \n",
    "    # Scale predicted probabilites from Machine Learning models\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    # Two-way interaction\n",
    "    poly = PolynomialFeatures(interaction_only=True) # Only include interactions, no squares\n",
    "    x_train = poly.fit_transform(x_train)\n",
    "    x_test = poly.transform(x_test)\n",
    "        \n",
    "     # Run models\n",
    "    model_time_start = time.time()\n",
    "#     model = LogisticRegression()\n",
    "    model = LogisticRegression(penalty = 'l1', solver = 'liblinear', C=5, l1_ratio =0)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "     # Collect and log evaluation metrics\n",
    "    auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "    model_time = time.time() - model_time_start\n",
    "\n",
    "    with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "        fd.write(f'Ensemble, {model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learner with 2-way Interactions (Structured + Pred + Under-sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 ....\n",
      "iter1 ....\n",
      "iter2 ....\n",
      "iter3 ....\n",
      "iter4 ....\n",
      "iter5 ....\n",
      "iter6 ....\n",
      "iter7 ....\n",
      "iter8 ....\n",
      "iter9 ....\n"
     ]
    }
   ],
   "source": [
    "pct = 0.5\n",
    "\n",
    "# Load the indices of the 10 undersampled sets\n",
    "pd_train_idx_pct = pd.read_csv(f'other3/pd_under_idx_10_pct_{pct}.csv')\n",
    "pd_test_idx_10 = pd.read_csv('other/pd_test_idx_10.csv') # Test sets remain the same\n",
    "\n",
    "pd_train_idx_pct.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "pd_test_idx_10.drop('Unnamed: 0', axis=1, inplace=True) \n",
    "\n",
    "# Load the predictions from the 5 models\n",
    "ML_pred_train_10 = pd.read_csv(f\"other2\\pd_pred_train_10_ML_Text_pct{pct}.csv\")\n",
    "ML_pred_test_10 = pd.read_csv(f\"other2\\pd_pred_test_10_ML_Text_pct{pct}.csv\")\n",
    "\n",
    "ML_pred_train_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "ML_pred_test_10.drop('Unnamed: 0', axis=1, inplace = True)\n",
    "\n",
    "\n",
    "with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "    fd.write(f'Undersample {pct} + Text Prediction + Ensemble Learner + remove SVM + interactions + LASSO\\n')\n",
    "\n",
    "for ii in range(10):\n",
    "    \n",
    "    train_index = pd_train_idx_pct.iloc[:, ii].values\n",
    "    test_index = pd_test_idx_10.iloc[:, ii].values\n",
    "    \n",
    "    iteration = \"iter\" + str(ii)\n",
    "    print(iteration, '....')\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Take the predictions from the 5 models\n",
    "    x_train = ML_pred_train_10.iloc[:,ii*5+0:ii*5+4]   \n",
    "    x_test = ML_pred_test_10.iloc[:,ii*5+0:ii*5+4]\n",
    "    \n",
    "    # Scale predicted probabilites from Machine Learning models\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    # Two-way interaction\n",
    "    poly = PolynomialFeatures(interaction_only=True) # Only include interactions, no squares\n",
    "    x_train = poly.fit_transform(x_train)\n",
    "    x_test = poly.transform(x_test)\n",
    "        \n",
    "     # Run models\n",
    "    model_time_start = time.time()\n",
    "#     model = LogisticRegression()\n",
    "    model = LogisticRegression(penalty = 'l1', solver = 'liblinear', C=5, l1_ratio =0)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "     # Collect and log evaluation metrics\n",
    "    auc_roc, auc_pr, acc, precision, recall, specificity, f1, tn, fp, fn, tp = evaluate(model, x_test, y_test)\n",
    "    model_time = time.time() - model_time_start\n",
    "\n",
    "    with open('structured_textpred/structured_textpred.csv','a') as fd:\n",
    "        fd.write(f'Ensemble, {model_names[jj]},{iteration},{auc_roc},{auc_pr},{acc},{precision},{recall},{specificity},{f1},{tn}, {fp}, {fn}, {tp},{model_time}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(),\n",
       "             param_grid={'C': [0.1, 1, 5, 10],\n",
       "                         'l1_ratio': [0, 0.3, 0.5, 0.7, 1],\n",
       "                         'penalty': ['l1', 'l2'],\n",
       "                         'solver': ['liblinear', 'sag', 'saga']})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg = GridSearchCV(cv=5,\n",
    "             estimator = LogisticRegression(),\n",
    "             param_grid={\"penalty\": [\"l1\", 'l2'], \n",
    "                         \"solver\": ['liblinear', 'sag', 'saga'],\n",
    "                         \"C\": [0.1, 1, 5, 10],\n",
    "                         \"l1_ratio\": [0, 0.3, 0.5, 0.7, 1]\n",
    "                        })\n",
    "lg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'C': 5, 'l1_ratio': 0, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters:\\n', lg.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Lasso(),\n",
       "             param_grid={'alpha': [0.5, 1, 5, 10],\n",
       "                         'fit_intercept': [True, False]})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_lasso = GridSearchCV(cv=5,\n",
    "             estimator = Lasso(),\n",
    "             param_grid={\"alpha\": [0.5, 1, 5, 10], \"fit_intercept\": [True, False]})\n",
    "lg_lasso.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5622, 0.0316, 0.9733827493261455, nan, 0.0, 1.0, nan)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(lg_lasso, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'alpha': 10, 'fit_intercept': True}\n"
     ]
    }
   ],
   "source": [
    "print('Best Parameters:\\n', lg_lasso.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "# Use Gridsearch to find the best parameters for the Random Forest model:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = GridSearchCV(cv=5,\n",
    "             estimator = RandomForestClassifier(min_samples_split = 5),\n",
    "             param_grid={\"criterion\": [\"entropy\", 'gini'], \"max_depth\": [5, 10, 20], 'max_features': ['auto', 'sqrt']})\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "print('Best Parameters:\\n', rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'bootstrap': [True, False],\n",
    "#  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "#  'max_features': ['auto', 'sqrt'],\n",
    "#  'min_samples_leaf': [1, 2, 4],\n",
    "#  'min_samples_split': [2, 5, 10],\n",
    "#  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2883,    6],\n",
       "       [  56,   23]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4993,\n",
       " 0.0014,\n",
       " 0.9957564505740159,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.99859411809408,\n",
       " nan,\n",
       " 26281,\n",
       " 37,\n",
       " 75,\n",
       " 0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(rf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'n_neighbors': 5, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# Use Gridsearch to find the best parameters for the KNN model:\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "knn = GridSearchCV(cv=5,\n",
    "             estimator = KNeighborsClassifier(),\n",
    "             param_grid={\"weights\": [\"distance\", 'uniform'], \"n_neighbors\": [5, 7, 9]})\n",
    "knn.fit(x_train, y_train)\n",
    "print('Best Parameters:\\n', knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4994,\n",
       " 0.0014,\n",
       " 0.9960595612473004,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.998898092560225,\n",
       " nan,\n",
       " 26289,\n",
       " 29,\n",
       " 75,\n",
       " 0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(knn, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'C': 0.1, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Use KernelPCA to change the data representation (project the original data to higher dimension)\n",
    "svm = GridSearchCV(cv=5,\n",
    "             estimator = SVC(kernel=\"rbf\"),\n",
    "             param_grid={\"C\": [0.025, 0.05, 0.1, 1, 10, 100, 1000], \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001]})\n",
    "\n",
    "svm.fit(x_train, y_train)\n",
    "print('Best Parameters:\\n', svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      " {'booster': 'gblinear', 'max_depth': 4, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "xgb = GridSearchCV(cv=5,\n",
    "             estimator = XGBClassifier(verbosity = 0),\n",
    "             param_grid={\"booster\": [\"gbtree\", 'gblinear','dart'], \"n_estimators\": [200, 300, 500], 'max_depth': [4,6,8]})\n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "print('Best Parameters:\\n', xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9828\n",
      "Best Parameters:\n",
      " {'booster': 'gblinear', 'max_depth': 4, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', round(xgb.score(x_test,y_test),4))\n",
    "print('Best Parameters:\\n', xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7682,\n",
       " 0.604,\n",
       " 0.980121293800539,\n",
       " 0.6515151515151515,\n",
       " 0.5443037974683544,\n",
       " 0.9920387677397023,\n",
       " 0.5931034482758619)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(xgb, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(booster = 'gblinear', n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gblinear', colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None,\n",
       "              enable_categorical=False, gamma=None, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.5, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=4, num_parallel_tree=None,\n",
       "              predictor=None, random_state=0, reg_alpha=0, reg_lambda=0,\n",
       "              scale_pos_weight=1, subsample=None, tree_method=None,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6103,\n",
       " 0.355,\n",
       " 0.9723719676549866,\n",
       " 0.46153846153846156,\n",
       " 0.22784810126582278,\n",
       " 0.9927310488058152,\n",
       " 0.3050847457627119)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilites from text classification with over- and under- sampling\n",
    "evaluate(xgb1, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2882,    7],\n",
       "       [  31,   48]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = xgb.predict(x_test)\n",
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.9870239e-08, -1.6471268e+00,  7.1076632e-02, -9.8611370e-02,\n",
       "       -1.1895554e-01, -1.0279215e-01, -9.6887641e-02, -4.6253982e-01,\n",
       "        1.3155596e-02,  8.6222373e-02, -7.4758483e-03, -9.8040737e-02,\n",
       "        8.4496766e-02, -2.2484398e-01,  3.3371112e-01, -2.6425568e-02,\n",
       "       -4.1604817e-01, -3.4293583e-01, -4.6148407e-01,  1.9643235e-03,\n",
       "        1.0709361e-01,  1.3362673e-01,  3.2788206e-02,  1.6523656e-01,\n",
       "        1.4395665e-01,  2.3232357e-01,  9.9862449e-02,  3.7412385e-03,\n",
       "       -4.6925820e-02,  7.5979762e-02,  2.3118609e-01, -5.6969807e-02,\n",
       "        1.0546323e-02, -8.5148225e-03, -1.2917040e-01,  3.8875345e-02,\n",
       "        7.2632488e-03,  2.1422903e-01,  1.2915637e-01,  3.3907312e-01,\n",
       "        2.0678045e-01,  2.6260731e-01,  2.1592718e-01,  1.7424393e-01,\n",
       "       -4.7747444e-02,  8.2987636e-01,  4.8537832e-02,  3.0805720e-03,\n",
       "       -1.9678801e-02, -4.1745353e-01,  1.3118086e-02,  4.3843467e-02,\n",
       "        3.9935850e-02, -1.5605908e-04, -1.7692487e-01,  2.5122157e-01,\n",
       "        3.8004008e-01,  2.0714992e-01,  1.7572454e-01,  1.9916710e-01,\n",
       "       -4.4510100e-02,  2.6842037e-02,  1.3489167e-01,  8.8989614e-03,\n",
       "        2.4811380e-01, -2.3983957e-01,  3.2627687e-02,  5.4755252e-02,\n",
       "       -7.0983849e-02,  2.6840809e-01, -8.2754262e-02, -1.5014588e-02,\n",
       "        8.7914035e-02, -8.4593305e-03], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "plot_importance(xgb1,max_num_features=20, show_values=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
